\documentclass{article}
\usepackage{xcolor}
\usepackage{titleps}
\usepackage[letterpaper, margin=0.95in]{geometry}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{tabu}
\usepackage{parskip}
\usepackage{natbib}
\usepackage{listings}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[hidelinks]{hyperref}       % hyperlinks
\usepackage{wrapfig}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath,amssymb} % define this before the line numbering.
\usepackage{makecell}
% % Support for easy cross-referencing
\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{colortbl}
\usepackage{xcolor}
% \usepackage{epsfig} % for postscript graphics files
\usepackage{empheq}
%\usepackage{mathptmx} % assumes new font selection scheme installed
% \usepackage{times} % assumes new font selection scheme installed
\usepackage{bm}
\usepackage{bbding} 
% \usepackage{cite}
\usepackage{diagbox}
\usepackage[linesnumbered,ruled]{algorithm2e}
% \usepackage{ulem} %to strike the words
% \usepackage{hyperref}
% \usepackage{soul}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
% \definecolor{themeblue}{RGB}{57, 162, 219}
% \definecolor{themegreen}{RGB}{87, 204, 153}
% \definecolor{forestgreen}{RGB}{47, 159, 87}

\usepackage[capitalize]{cleveref}
% \usepackage{todonotes}
\usepackage{float}
\usepackage{booktabs}
\usepackage{multirow}
%\usepackage{ bbold }
\usepackage{mathrsfs}
\usepackage[utf8]{inputenc}
%\usepackage{subfigure}
\usepackage{pifont}
\usepackage{threeparttable}

\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}

\usepackage{hyperref}
\usepackage[color=red]{todonotes}
\usepackage{forest}
\definecolor{light-yellow}{HTML}{FFE5CC}
\newcounter{RNum}
\renewcommand{\theRNum}{\arabic{RNum}}
\newcommand{\Remark}{\noindent\textbf{Remark}~\refstepcounter{RNum}\textbf{\theRNum}: }
\newcommand{\fref}[1]{Fig.~\ref{#1}}
\newcommand{\sref}[1]{Section~\ref{#1}}
\newcommand{\tref}[1]{Table~\ref{#1}}
\newcommand{\appref}[1]{Appendix~\ref{#1}}
\newcommand{\highlight}[1]{\noindent\quad\textbf{#1}:~}
\newcommand{\myparagraph}[1]{\noindent\textbf{#1}~}

\newpagestyle{ruled}
{\sethead{CMU 16-831}{Intro to Robot Learning}{Fall 2024}\headrule
  \setfoot{}{}{}}
\pagestyle{ruled}

\renewcommand\makeheadrule{\color{black}\rule[-.75\baselineskip]{\linewidth}{0.4pt}}
\renewcommand*\footnoterule{}

\begin{document}
\lstset{basicstyle = \ttfamily,columns=fullflexible,
backgroundcolor = \color{light-yellow}
}

\begin{centering}
    {\Large Assignment 1: Imitation Learning} \\
    \vspace{.25cm}
    \textbf{Andrew ID:} \texttt{micahn} \\
    \textbf{Collaborators:} \texttt{portegak, giria, nludlow}\\ 
\end{centering}

\vspace{.5cm}

\section{Behavioral Cloning (65 pt)}
\subsection{Part 2 (10 pt)}

To get the following results in Table \ref{tab:p2}, run the respective command in `RUN.md` to replicate results.


\begin{table}[!h]
  \centering
  \caption{Report your result in this table.}
    \begin{tabular}{cccccc}
    \toprule[1.0pt]
    Metric/Env & Ant-v2 & Humanoid-v2 & Walker2d-v2 & Hopper-v2 & HalfCheetah-v2 \\
    \midrule
    Mean  & 4713.653 & 10344.518 & 5566.846 & 3772.670 & 4205.778 \\
    Std.  & 12.197 & 20.981 & 9.238  & 1.948  & 83.039 \\
    \bottomrule[1.0pt]
    \end{tabular}%
    \caption{Expert and standard deviation of policy's return. Key parameters used are ep\_len of 1000, num\_agent\_train\_steps\_per\_iter of 1000, n\_iter of 1, batch\_size of 1000, eval\_back\_size of 1000, train\_batch size of 100, n\_layers of 2, size of 64, learning rate of 0.005.}
  \label{tab:p2}%
\end{table}%

\subsection{Part 3 (35 pt)}
\tref{tab:p3} shows the results when tuning hyperparameters for the Ant-v2 environment and leaving these hyperparameters for Humanoid-v2. Run the respective command in `RUN.md` to replicate results.

\begin{table}[htbp]
  \centering
  \caption{Fill your results in this table.}
    \begin{tabular}{ccccc}
    \toprule[1.0pt]
    Env   & \multicolumn{2}{c}{Ant-v2} & \multicolumn{2}{c}{Humanoid-v2} \\
    \midrule
    Metric & Mean  & Std.  & Mean  & Std. \\
    Expert & 4713.653 & 12.197 & 10344.518 & 20.981 \\
    BC    & 4654.067 & 76.135 & 300.748 & 47.048 \\
    \bottomrule[1.0pt]
    \end{tabular}%
  \label{tab:p3}%
\end{table}%

\subsection{Part 4 (20 pt)}
Fig. \ref{fig:p4} shows how the Behavior Clone's performance varies with respect to number of training steps. To get the following results in Fig. \ref{fig:p4}, run the respective command in `RUN.md` to replicate results.

\begin{figure*}[!h]
	\centering
\includegraphics[width=0.5\columnwidth]{bc_performance_vs_tsteps.png}
	\caption{BC agent’s performance (mean and standard deviation) varies with the value of the training steps parameter in Ant v2 environment. I chose this parameter because I know in any learning context, the number of training steps or epochs a model has to learn from the data and descend in loss, the better it becomes, until performance plateaus or saturates.}
	\label{fig:p4}
\end{figure*}

\clearpage
\section{DAgger (35 pt)}
\subsection{Part 2 (35 pt)}
\fref{fig:p5} shows the learning curve results of the DAgger algorithm. DAgger similarly or outperforms the expert, with no standard deviation at all for Ant-v2. For Humanoid-v2, DAgger matches the Behavior Clone data at best but vastly underperforms the expert.  Run the respective command in `RUN.md` to replicate results.

\begin{figure*}[!ht]
	\centering
        \begin{tabular}{cc}
           \includegraphics[width=0.5\columnwidth]{dagger_ant.png}  & 
           \includegraphics[width=0.5\columnwidth]{dagger_humanoid.png}
        \end{tabular}

	\caption{Learning curve, plotting the number of DAgger iterations vs. the policy’s mean return, with error bars to show the standard deviation.  The Ant-v2 environment performance is in the left plot and the Humanoid-v2 environment performance is in the right plot. Key parameters used are ep\_len of 1000, num\_agent\_train\_steps\_per\_iter of 1000, n\_iter of 10, batch\_size of 1000, eval\_back\_size of 1000, train\_batch size of 100, n\_layers of 2, size of 64, learning rate of 0.005.}
	\label{fig:p5}
\end{figure*}

\end{document}